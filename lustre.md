# Lustre

Взято [отсюда](https://ru.wikipedia.org/wiki/Lustre_%28%D1%81%D0%B5%D1%82%D0%B5%D0%B2%D0%B0%D1%8F_%D1%84%D0%B0%D0%B9%D0%BB%D0%BE%D0%B2%D0%B0%D1%8F_%D1%81%D0%B8%D1%81%D1%82%D0%B5%D0%BC%D0%B0%29).

## Архитектура


Файловая система Lustre содержит три основных функциональных модуля:

* Один сервер метаданных (metadata server — MDS) соединенный с одной целью метаданных (metadata target — MDT) в файловой системе Lustre, который хранит метаданные о пространстве имен, например имена файлов, каталогов, права доступа, а также карту размещения файлов. Данные MDT хранятся в единой локальной дисковой файловой системе.

* Один или несколько серверов хранения объектов (object storage server — OSS), хранящих данные файлов из одного или нескольких целей хранения объектов (object storage targets — OST). В зависимости от оборудования сервера OSS обычно обслуживает от двух до восьми OSs, а каждая OST управляет одной локальной дисковой файловой системой. Ёмкость файловой системы Lustre определяется суммой ёмкостей, предоставляемых OST.

* Клиент(ы), обращающиеся и использующие данные. Lustre предоставляет всем клиентам унифицированное пространство имен для всех файлов и данных в файловой системе, используя стандартную семантику POSIX, а также обеспечивает параллельный когерентный доступ по записи и чтению к файлам в файловой системе.

Компоненты MDT, OST, а также клиенты могут находиться на одном и том же узле, но при инсталляции их обычно помещают на отдельные узлы, взаимодействующие по сети. Уровень сети Lustre (Lustre Network — LNET) поддерживает несколько коммуникационных платформ, включая Infiniband, TCP/IP через Ethernet и другие сети, Myrinet, Quadrics, а также проприетарные сетевые технологии. Lustre также использует преимущества удаленного прямого доступа к памяти, если это возможно, для увеличения пропускной способности и снижения доли использования ЦП.

Хранилище, используемое для дублирующих файловых систем MDT и OST, делится на части и может быть организовано через управление логическими томами и/или RAID, как правило, форматированными в файловой системе ext4. Сервера Lustre OSS и MDS считывают, записывают и изменяют данные в формате, введённым этими файловыми системами.

OST является выделенной файловой системой, экспортирующей интерфейс в байтовые регионы объектов для операций чтения/записи. MDT является выделенной файловой системой для контроля доступа к файлам и сообщений киентам о том, какие объект(ы) входят в структуру файла. В настоящее время MDT и OST используют для хранения данных улучшенную версию ext4, называемую ldiskfs. В 2008 Sun начала проект портирования Lustre в Sun ZFS/DMU для хранения внутренних данных, продолжающийся как проект с открытым кодом.

При доступе клиента к файлу поиск имени файла выполняется в MDS. В результате файл либо создается от имени клиента, либо клиенту возвращается местоположение существующего файла. При операциях чтения или записи клиент интерпретирует местоположение на уровне логического тома объекта, отображающего смещение и размер в один или несколько объектов, каждый из которых расположен на отдельной OST. Затем клиент блокирует диапазон файлов для работы и исполняет одну или несколько операций параллельного чтения или записи непосредственно в OST. При таком подходе устраняются узкие места взаимодействия клиента с OST, так что общая пропускная способность, доступная клиенту для чтения и записи, масштабируется почти линейно с ростом количества OST в файловой системе.

Клиенты не модифицируют объекты файловой системы OST напрямую, делегируя эту задачу OSS. Этот подход обеспечивает масштабируемость для крупномасштабных кластеров и суперкомпьютеров, а также улучшает безопасность и надежность. В противоположность этому блочные разделяемые файловые системы, например Global File System и OCFS должны поддерживать прямой доступ к хранилищу для всех клиентов в файловой системе, увеличивая риск повреждения файловой системы со стороны неправильных клиентов.

## Реализация

При обычной установке Lustre на клиенте Linux модуль драйвера файловой системы Lustre загружается в ядро и файловая система монтируется как и любая другая локальная или сетевая файловая система. Приложения клиента видят одну унифицированную файловую систему, даже если она может быть составлена из дестяков тысяч индивидуальных серверов и файловых систем MDT/OST.

На некоторых процессорах массового параллелизма (MPP) вычислительные процессоры могут получить доступ к файловой системе Lustre путём перенаправления их запросов ввода-вывода на выделенный узел ввода-вывода, сконфигурированный как клиент Lustre. Этот подход используется в Blue Gene[20], установленном в Ливерморской национальной лаборатории.

Другой подход, используемый в последнее время, предлагает библиотека liblustre, предоставляющая приложениям в пространстве пользователя прямой доступ к файловой системе. Это библиотека уровня пользователя, позволяющая вычислительным процессорам-клиентам монтировать и использовать файловую систему Lustre. Используя liblustre, вычислительные процессоры могут получить доступ к файловой системе Lustre даже в случае, если обслуживающий узел, запустивший задание, не является клиентом Lustre. Библиотека liblustre позволяет непосредственно перемещать данные между пространством приложения и Lustre OSS без необходимости промежуточного копирования данных через ядро, обеспечивая вычислительным процессорам доступ к файловой системе Lustre с низкой задержкой и высокой пропускной способностью.

## Объекты данных и разделение данных

В традиционных дисковых файловых системах UNIX, структура данных inode содержит базовую информацию о каждом файле, например о том, где хранятся данные, содержащиеся в файле. Файловая система Lustre также использует inode, но inode на MDT указывает на один или несколько объектов OST, ассоциированных с файлом, а не на блоки данных. Эти объекты реализуются как файлы OST. При открытии файла клиентом операция открытия передает множество указателей объекта и их расположение из MDS клиенту, после чего клиент может непосредственно взаимодействовать с узлом OSS, хранящим объект, что позволяет клиенту выполнять ввод-вывод в файл без последующего взаимодействия с MDS.

Если с MDT inode ассоциирован только один объект OST, этот объект содержит все данные файла Lustre. Если с файлом ассоциировано более одного объекта, данные файла «разделены» среди объектов подобно RAID 0. Разделение файла между несколькими объектами дает существенные преимущества в производительности. При использовании разделения, максимальный размер файла не ограничен размерами одной цели. Ёмкость и совокупная пропускная способность ввода-вывода масштабируется с ростом числа OST, по которым разделен файл. Кроме того, поскольку блокировка каждого объекта для каждой OST управляется независимо, добавление частей (OST) масштабирует возможности блокировки ввода-вывода в файл пропорционально. Каждый файл в файловой системе может иметь различное размещение при разделении, так что ёмкость и производительность можно оптимально настроить для каждого файла.

## Блокировки

Lustre использует менеджера распределённой блокировки в стиле VMS для защиты целостности данных и метаданных каждого файла. Доступ и модификация файла Lustre полностью когерентна для всех клиентов. Блокировки метаданных управляются MDT, хранящим inode файла с использованием 128-битного идентификатора Lustre File Identifier (FID, состоящего из номера последовательности и идентификатора объекта), используемого как имя ресурса. Блокировки метаданных делятся на несколько частей, защищающих процесс обнаружения файла (владелец и группа файла, разрешения и режим, а также ACL, состояние inode (размер каталога, содержимое каталога, количество ссылок, временные метки), а также размещение (разделение файла). Клиент может получить несколько частей блокировки метаданных для одного inode одним запросом RPC, но в настоящее время предоставляются только блокировки для чтения inode. MDS управляет всеми модификациями inode, чтобы избежать конкуренции за ресурс и сейчас возможен только один узел, получающий блокировки на запись inode.

Блокировки данных файла управляются OST, по которым разделен каждый объект файла, с использованием байтовых блокировок экстентов. Клиенты могут получить перекрывающиеся блокировки чтения экстента для части или всего файла, что позволяет существовать нескольким параллельным читателям для одного файла, а также неперекрывающиеся блокировки записи экстента для областей файла. Это позволяет многим клиентам Lustre получить параллельный доступ к файлу для чтения и записи, избегая проблем при вводе-выводе файла. На практике клиенты Linux управляют своим кэшем данных в единицах страниц, поэтому они будут запрашивать блокировки, всегда являющиеся целым, кратным размеру страницы (4096 байт в большинстве клиентов). При запросе клиентом блокировки экстента OST может предоставить блокировку на больший экстент, чтобы сократить количество запросов блокировки, выполняемых клиентом. Реальный размер предоставленной блокировки зависит от нескольких факторов, включая количество уже выполненных блокировок, наличие конфликтующих блокировок на запись, и числа запросов на блокировку. Установленная блокировка никогда не будет меньше требуемого экстента. Блокировки экстента OST используют Lustre FID в качестве имени ресурса для блокировки. Поскольку число серверов блокировки экстента растет с увеличением числа OST в файловой системе, это также увеличивает совокупную производительность блокировки файловой системы и одного файла, если он разделен по нескольким OST.

## Высокая доступность

Высокая доступность файловой системы Lustre включает надежный механизм отказоустойчивости и восстановления, обеспечивающий прозрачную перезагрузку серверов при неисправности. Взаимодействие версий между младшими версиями программного обеспечения Lustre позволяет обновить сервер, выключив его (или переключив на резервный сервер), выполнить обновление и перезапустить сервер, причем все активные задания продолжают работать, обнаруживая задержку только при переводе хранилища на резервный сервер.

Lustre MDS конфигурируются как активная/пассивная пара, а OSS обычно развертываются в конфигурации активный/активный, обеспечивающей надежность без существенной перегрузки. Часто резервный MDS является активным MDS для другой файловой системы Lustre, поэтому в кластере нет простаивающих узлов.


## Пример создания простейшего кластера на базе Lustre

### Установка системы

Вначале необходимо настроить 3-5 виртуалок (под `MGT`, `MDT` и `OST's` и клиент).
На текущий момент проще всего поставить в качестве операционной системы `CentOS 7` (ядро 3.10):

```bash
[root@centos7-lustre-1 ~]# uname -a
Linux centos7-lustre-1 3.10.0-327.el7.x86_64 #1 SMP Thu Nov 19 22:10:57 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux
```

Систему можно ставить с дефолтными настройками, только лишь надо будет добавить `epel` для доступа к дополнительным пакетам.

Теперь надо поставить ZFS. Установка zfs на `rpm`-образные дистрибутивы описана в офф. вики ZoL:

```bash
yum localinstall –nogpgcheck http://archive.zfsonlinux.org/epel/zfs-release$(rpm -E %dist).noarch.rpm
yum check-update
yum install zfs
```

Там также отмечено, что есть два типа пакетов - kmod и dkms. Лучше выбирать dkms.
Это может пригодиться, если у вас будет устанавливаться пропатченное ядро (но в данном случае мы этого делать не будем).
Кроме того, стоит отметить, что в данной версии бытые ссылки на хидеры. Необходимо добиться, чтобы содержимое `/usr/src`
выглядело так:

```bash
[root@centos7-lustre-1 ~]# ll /usr/src/
итого 12
drwxr-xr-x.  2 root root    6 авг 12  2015 debug
drwxr-xr-x.  3 root root   67 авг 16 06:09 kernels
drwxr-xr-x  11 root root 4096 авг 16 06:33 lustre-2.8.0
drwxr-xr-x. 10 root root 4096 авг 16 06:09 spl-0.6.5.7
drwxr-xr-x  13 root root 4096 авг 16 06:29 zfs-0.6.5.7
[root@centos7-lustre-1 ~]# ll /usr/src/kernels/
итого 4
drwxr-xr-x. 22 root root 4096 авг 16 06:07 3.10.0-327.28.2.el7.x86_64
lrwxrwxrwx   1 root root   43 авг 16 06:09 3.10.0-327.el7.x86_64 -> /usr/src/kernels/3.10.0-327.28.2.el7.x86_64
```

То есть необходимо вручную добавить ссылку `3.10.0-327.el7.x86_64 -> /usr/src/kernels/3.10.0-327.28.2.el7.x86_64` в `/usr/src/kernels`.
Возможно, есть и другой способ заставить работать `dkms`.

Итак, для пользователя ставим `libzfs*` (который в качестве зависимостей тянет `libnvpair*`, `libuutil*` и другие).
Для ядра необходимо поставить `zfs-dkms` и `spl-dkms`.

После того, как `dkms`-модули `ZFS` и `SPL` установятся, можно приступить к установке самой `Lustre`.

Вначале создадим файл `/etc/yum.repos.d/lustre.repo` со следующим содержимым:

```conf
[lustre-server]
name=CentOS-$releasever - Lustre
baseurl=https://downloads.hpdd.intel.com/public/lustre/latest-feature-release/el7/server/
gpgcheck=0

[e2fsprogs]
name=CentOS-$releasever - Ldiskfs
baseurl=https://downloads.hpdd.intel.com/public/e2fsprogs/latest/el7/RPMS
gpgcheck=0

[lustre-client]
name=CentOS-$releasever - Lustre
baseurl=https://downloads.hpdd.intel.com/public/lustre/latest-feature-release/el7/client/
gpgcheck=0
```

Затем нам необходимо установить компоненты lustre, в том числе и lustre-dkms.


### Настройка системы

Теперь необходимо прописать конфиги в `/etc/ldev.conf` и скопировать их на все машины. Например:

```bash
[root@centos7-lustre-1 ~]# cat /etc/ldev.conf 
#local  foreign/-  label       [md|zfs:]device-path   [journal-path]/- [raidtab]
#1.              2. 3.      4.
centos7-lustre-1 -- mgs     zfs:warp-mgt0/mgt0
centos7-lustre-1 -- mdt     zfs:warp-mdt0/mdt0
centos7-lustre-2 -- ost0    zfs:warp-ost0/ost0
```

Где показаны:

1. Имя хоста на котором должно находиться хранилище

2. Имя запасного хоста (куда надо реплицировать данные, в моём случае `"--"`, т.е. нет реплики).

3. Тип хранилища.

4. Имя датасета, на который хранит данные (`<type>:<pool>/<dataset>`).

Теперь сделаем несколько клонов этой машины.

### Создание ФС

Вначале создаём MGT-хранилище:

```
mkfs.lustre --mgs --backfstype=zfs warp-mgt0/mgt0 /tmp/lustre.file
```

Затем - MDT:

```
IP=172.16.8.140 # mgs ip
mkfs.lustre --mdt --backfstype=zfs --index=0  --mgsnode=${IP}@tcp --fsname warpfs warp-mdt0/mdt0 /tmp/mdt0.file
```

Стоит отметить, что хранилища могут размещаться на разных нодах и на разных пулах. Могут и на одном и том же пуле одной ноды.
Пулы также можно создавать любым образом.

Теперь можно запустить сервис `lustre`. Стоит отметить, что в примере пулы создаются поверх файлов, так что рестарт сервиса несработает,
так как нужно будет вручную импортировать пулы.

```
service lustre start
```

Теперь создаём OST-хранилище (здесь будут лежать данные). Создаём его на соседней ноде:

```bash
IP=172.16.8.140 # mgs ip
mkfs.lustre --ost --backfstype=zfs --index=0 --mgsnode=${IP}@tcp --fsname warpfs warp-ost0/ost0 /tmp/lustre.file
```

На каждой ноде после `mkfs` выполняем запуск сервиса `lustre`.


### Монтирование ФС

Монтирование ФС `Lustre` можно выполнить следующим образом:

```bash
IP=172.16.8.140 # mgs ip
mount -t lustre ${IP}@tcp:/warpfs /warpfs
```


